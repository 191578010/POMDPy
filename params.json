{
  "name": "POMDPy",
  "tagline": "POMDPs in Python",
  "body": "## POMDPy\r\n![Build](https://travis-ci.org/pemami4911/POMDPy.svg?branch=master)  ![Python27](https://img.shields.io/badge/python-2.7-blue.svg)  ![Python35](https://img.shields.io/badge/python-3.5-blue.svg)\r\n\r\nThis open-source project contains a framework for implementing discrete action/state POMDPs in Python. This work was inspired by [TAPIR](http://robotics.itee.uq.edu.au/~hannakur/dokuwiki/doku.php?id=wiki:tapir) and the [POMCP](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications.html) algorithm.\r\n\r\n[What the heck is a POMDP?](http://www.pomdp.org/tutorial/index.shtml)\r\n\r\nHere's David Silver and Joel Veness's paper on POMCP, a ground-breaking POMDP solver. [Monte-Carlo Planning in Large POMDPs](http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf)\r\n\r\nThis project has been conducted strictly for research purposes. If you would like to contribute to POMDPy or if you have any comments or suggestions, feel free to send me a pull request or send me an email at pemami@ufl.edu.  \r\n\r\n## Dependencies ##\r\nDownload the files as a zip or clone into the repository.\r\n\r\n    git clone https://github.com/pemami4911/POMDPy.git\r\n\r\nThis project uses:\r\n\r\n* numpy >= 1.11\r\n* matplotlib >= 1.4.3\r\n* scipy >= 0.15.1\r\n* future >= 0.16\r\n* tensorflow >= 0.12\r\n\r\nSee the [Tensorflow docs](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#download-and-setup) for information about installing Tensorflow. \r\n\r\nThe easiest way to satisfy the dependencies is to use Anaconda. You might have to run `pip install --upgrade future` after installing Anaconda, however. \r\n\r\n## Supported Solvers ##\r\n\r\n* [POMCP](https://github.com/pemami4911/POMDPy/blob/master/pomdpy/solvers/pomcp.py)\r\n* [SARSA](https://github.com/pemami4911/POMDPy/blob/master/pomdpy/solvers/sarsa.py)\r\n* [Value Iteration](https://github.com/pemami4911/POMDPy/blob/master/pomdpy/solvers/value_iteration.py)\r\n\r\nThe main difference between POMCP and SARSA is that POMCP uses the off-policy Q-Learning\r\nalgorithm and the UCT action-selection strategy. SARSA uses an on-policy variant of TD-Learning. **Both algorithms \r\nencode the action-value function as a belief search tree.** POMCP is an anytime planner that approximates the action-value\r\nestimates of the current belief via Monte-Carlo simulations before taking a step. This is known as Monte-Carlo Tree Search (MCTS).\r\nSARSA is episodic, in that the agent repeatedly carries out full episodes \r\nand uses the generated history of experiences to back-up the action-value estimates up the taken path to the root of the belief tree. \r\n\r\nI have also implemented exact Value Iteration with Lark's pruning algorithm. This can only be used on the Tiger Problem. \r\n\r\n## Running an example ##\r\nCurrently, you can test POMCP and SARSA on the classic Tiger and RockSample POMDPs. \r\n\r\nYou can optionally edit the RockSample configuration file `rock_problem_config.json` to change the map size or environment parameters.\r\nThis file is located in `pompdy/config`.\r\nThe following maps are available:\r\n* RockSample(7, 8), a 7 x 7 grid with 8 rocks.\r\n* RockSample(11, 11), an 11 x 11 grid with 11 rocks\r\n* RockSample(15, 15), a 15 x 15 grid with 15 rocks\r\n* As well as a few others, such as (7, 2), (7, 3), (12, 12), and more. It is fairly easy to make new maps.\r\n\r\nTo run the RockSample problem with POMCP:\r\n\r\n    ./main.py --env RockProblem --solver POMCP --max_steps 200 --epsilon_start 1.0 --epsilon_decay 0.01 --n_runs 10 --n_sims 500  --preferred_actions --seed 123\r\n        \r\nTo run the Tiger problem with SARSA: \r\n\r\n    ./main.py --env TigerProblem --solver SARSA --max_steps 5 --epsilon_start 0.5 --n_runs 100 --seed 123\r\n       \r\nSee `pompdy/README.md` for details about implementing new POMDP benchmark problems.\r\n    \r\n## TODO ##\r\n* [ ] Random baseline solver\r\n* [ ] Add more unit tests\r\n* [ ] Add additional benchmark problems \r\n* [ ] Continuous-action/state space POMDPs\r\n",
  "google": "82749305",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}